{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1). Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2). Load Data\n",
    "### a). test.csv has only featurs and no labels and is used to evaluate the performace, corrosponding score will be highlighted on the leaderboard as per Kaggle rules\n",
    "### b). train.csv has both labels and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load test data\n",
    "with open(\"../input/test.csv\",\"rb\") as f:\n",
    "    temp_data = pd.read_csv(f)\n",
    "#test_labels = np.asarray(temp_data[\"label\"])\n",
    "test_data = np.asarray(temp_data)\n",
    "\n",
    "#load train data and labels\n",
    "with open(\"../input/train.csv\",\"rb\") as f:\n",
    "    temp_data = pd.read_csv(f)\n",
    "train_labels = np.asarray(temp_data[\"label\"])\n",
    "train_data = np.asarray(temp_data.drop(columns=\"label\"))\n",
    "\n",
    "#val_count = np.floor_divide(30*len(train_labels),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3).  stratified split\n",
    "### Perform a stratified shuffle split into the train dataset to create train (80%) and crossvalidation (20%) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index,test_index in split.split(train_data,train_labels):\n",
    "    strat_train_data, strat_val_data = train_data[train_index], train_data[test_index]\n",
    "    strat_train_labels, strat_val_labels = train_labels[train_index], train_labels[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4). Image variables and directories\n",
    "### a). \"height\" and \"width\" define the  image pixel dimensions and \"channel\" define the image color depth (= 1 for black and white image).\n",
    "### b). \"proj_dir\": Current project directory where the model will be saved; \"checkpoint_path\": Variable to store the intermediate checkpoint which will help to resume the model if the run terminate abruptly; \"checkpoint_epoch_path\": Variable to store the intermediate checkpoint epoch number; \"final_model_path\": Variable to store the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define image dimensions which is 28x28x1 for MNIST dataset\n",
    "# \"height\" and \"width\" define the  image pixel dimensions\n",
    "height = 28\n",
    "width = 28\n",
    "#\"channel\" define the image color depth (= 1 for black and white image)\n",
    "channel = 1\n",
    "\n",
    "#Current project directory where the model will be saved\n",
    "proj_dir = \"../working/character_classification_akj/\"\n",
    "#Variable to store the intermediate checkpoint which will help to resume the model if the run terminate abruptly\n",
    "checkpoint_path = proj_dir + \"intermediate_checkpoint.ckpt\"\n",
    "#Variable to store the intermediate checkpoint epoch number\n",
    "checkpoint_epoch_path = checkpoint_path+\".epoch\"\n",
    "#Variable to store the best model\n",
    "final_model_path = proj_dir + \"my_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5). fetch_batch: Function to get shuffled training data and labels of specified batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(feature_set, labels, batch_size):\n",
    "    p = np.random.permutation(len(feature_set))\n",
    "    return (feature_set[p][:batch_size,:], labels[p][:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6). Graph definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# placeholder for batch training data\n",
    "X = tf.placeholder(tf.float32, [None,height*width*channel], name=\"X\")\n",
    "# placeholder for batch training labels\n",
    "y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "# placeholder for dropout; differentiate \"training\" from \"crossvalidation\" and \"prediction\"\n",
    "training = tf.placeholder_with_default(False,shape=(), name = \"training_variable\")\n",
    "# placeholder to perform learning rate scheduling\n",
    "learning_rate = tf.placeholder_with_default(0.01,shape=(), name= \"learning_rate\")\n",
    "global_step_tensor = tf.Variable(0,trainable=False, name= \"global_step\")\n",
    "\n",
    "# reshape the input \n",
    "input_layer = tf.reshape(X, [-1,height,width,channel], name=\"Input_layer\")\n",
    "# layer_1: convolution\n",
    "conv1 = tf.layers.conv2d(inputs= input_layer,\n",
    "                         filters= 32,\n",
    "                         kernel_size= [5,5],\n",
    "                         padding= \"same\",\n",
    "                         activation= tf.nn.relu,\n",
    "                         name= \"conv_1\")\n",
    "\n",
    "# layer_2: Maxpool\n",
    "pool1 = tf.layers.max_pooling2d(inputs= conv1,\n",
    "                                pool_size= [2,2],\n",
    "                                strides= 2,\n",
    "                                name= \"pool_1\")\n",
    "\n",
    "# layer_3: convolution\n",
    "conv2 = tf.layers.conv2d(inputs= pool1,\n",
    "                         filters= 64,\n",
    "                         kernel_size= [5,5],\n",
    "                         padding= \"same\",\n",
    "                         activation= tf.nn.relu,\n",
    "                         name= \"conv_2\")\n",
    "    \n",
    "# layer_4: Maxpool\n",
    "pool2 = tf.layers.max_pooling2d(inputs= conv2,\n",
    "                                pool_size= [2,2],\n",
    "                                strides= 2,\n",
    "                                name= \"pool_2\")\n",
    "\n",
    "# flatten the layer_4 to apply to ANN\n",
    "pool2_flat = tf.reshape(pool2, [-1,7*7*64])\n",
    " \n",
    "# layer_5: fully connected   \n",
    "dense = tf.layers.dense(inputs= pool2_flat,\n",
    "                        units= 1024,\n",
    "                        activation= tf.nn.relu)\n",
    "\n",
    "# layer_5: dropout\n",
    "dropout = tf.layers.dropout(dense,\n",
    "                            rate=0.3,\n",
    "                            training=training,\n",
    "                            name=\"Dropout\")\n",
    "\n",
    "# output layer\n",
    "logits = tf.layers.dense(inputs= dropout, units= 10)\n",
    "#classes = tf.argmax(logits,axis=1)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                          labels=y,\n",
    "                                                          name=\"xentropy\")\n",
    "loss = tf.reduce_mean(xentropy,\n",
    "                      name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss,global_step=global_step_tensor)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exponentially decaying learning rate\n",
    "learning_rate_array = 0.0001*np.exp(-20000*np.linspace(0.000001,0.001,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7). Model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "old_loss = np.inf\n",
    "new_loss = 0\n",
    "early_stop_count = 0\n",
    "\n",
    "n_epoch = 10\n",
    "n_iter = 1000\n",
    "batch_size = 200\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path) as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interupted resuming from epoch \", start_epoch)\n",
    "        saver.restore(sess,checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        init.run()\n",
    "        os.mkdir(\"../working/character_classification_akj\")\n",
    "    for epoch in range(start_epoch,n_epoch):\n",
    "        learning_rate_epoch = learning_rate_array[epoch]\n",
    "        for iteration in range(n_iter):\n",
    "            X_batch, y_batch = fetch_batch(strat_train_data, strat_train_labels, batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch,\n",
    "                                             y:y_batch,\n",
    "                                             training:True,\n",
    "                                             learning_rate:learning_rate_epoch})\n",
    "            new_loss = sess.run(loss,feed_dict={X:strat_val_data,y:strat_val_labels})\n",
    "            if new_loss<old_loss:\n",
    "                saver.save(sess,final_model_path)\n",
    "                old_loss = new_loss\n",
    "                best_epoch = epoch\n",
    "                best_iteration = iteration\n",
    "                early_stop_count=0\n",
    "            else:\n",
    "                early_stop_count+=1\n",
    "                if early_stop_count>500:\n",
    "                    print(\"Early stopping satisfied with best epoch \",best_epoch,\n",
    "                          \" and best iteration \",iteration)\n",
    "                    current_train_loss, current_train_accu = sess.run([loss,accuracy],\n",
    "                                                                      feed_dict={X:strat_train_data,\n",
    "                                                                                y:strat_train_labels})\n",
    "                    current_val_loss, current_val_accu = sess.run([loss,accuracy],\n",
    "                                                                      feed_dict={X:strat_val_data,\n",
    "                                                                                y:strat_val_labels})\n",
    "                    print(\"Current train loss: \",current_train_loss,\n",
    "                          \" Current train accuracy: \", current_train_accu)\n",
    "                    print()\n",
    "                    print(\"Current val loss: \",current_val_loss,\n",
    "                          \" Current val accuracy: \", current_val_accu)\n",
    "                    print()\n",
    "                    print(\"Best model loss: \", old_loss)\n",
    "                    break\n",
    "        if early_stop_count>50:\n",
    "            break\n",
    "        if epoch%1==0:\n",
    "            print(\"Saving checkpoint for epoch \",epoch)\n",
    "            saver.save(sess,checkpoint_path)\n",
    "            with open(checkpoint_epoch_path,\"wb\") as f:\n",
    "                f.write(b'%d' % (epoch+1))\n",
    "            train_loss, train_accuracy = sess.run([loss, accuracy],\n",
    "                                                    feed_dict={X:X_batch, y:y_batch})\n",
    "            train_loss_list.append(train_loss)\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "            print(\"Epoch: \",epoch)    \n",
    "            print(\"Training loss: \",train_loss,\" Training accuracy: \",train_accuracy)\n",
    "            print()\n",
    "    #saver.save(sess,final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8). Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,final_model_path)\n",
    "    predict = sess.run(logits,feed_dict={X:test_data})\n",
    "    predict_class = sess.run(tf.argmax(predict,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9). Generation of \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = \"submission.csv\"\n",
    "with open(output_file, 'w') as f :\n",
    "    f.write('ImageId,Label\\n')\n",
    "    for i in range(len(predict_class)) :\n",
    "        f.write(\"\".join([str(i+1),',',str(predict_class[i]),'\\n']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
